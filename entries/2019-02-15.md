> New AI fake text generator [may be too dangerous to release, say creators](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction).

The title is sensationalist, as most are, but signals a unique type of hesitation. “This is good. So good. In fact, it’s too good.”[^1]

There is a [a call to open source the language model
](https://thegradient.pub/openai-please-open-source-your-language-model/), arguing historical fears of new “*deceitful*” technology leads to a general ability to identify instances of misuse. It’s important to not confuse normalization for education. Just because we grow used to it does not mean we understand it.

While it’s difficult to pinpoint the motivation of language in a press release, I wonder how the individual researcher relates to the hesitation within their own practice.

How does one justify a gravitational fascination with interesting work when what makes it fascinating to the individual is perhaps dangerous at scale?

[^1]: Github also now prevents unauthenticated visitors from accessing the repository for [`deepfakes`](https://github.com/deepfakes/faceswap).